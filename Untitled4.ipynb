{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2AEEwhBKssDLOxT6kxVDZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "87bbe96e70854624837ddf009990f3ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7511ee490724b10aca8329cb1d7656c",
              "IPY_MODEL_f892209085ad433482e044155e8d85ce",
              "IPY_MODEL_83fe12f693a84312a335ea1b8fbf9768"
            ],
            "layout": "IPY_MODEL_c0444c2bd2c64f1587151318d3461127"
          }
        },
        "c7511ee490724b10aca8329cb1d7656c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6d0d43bb6664f129209f3e29ca9dd84",
            "placeholder": "​",
            "style": "IPY_MODEL_63c13b3ae53f4cd7bb90bd276f99dfb0",
            "value": "Best trial: 17. Best value: 10.5216:  38%"
          }
        },
        "f892209085ad433482e044155e8d85ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8f5200ec5a04c7ea7223061b8e1e8ab",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b45de894d1634977b1bdd0cd3a0aa856",
            "value": 19
          }
        },
        "83fe12f693a84312a335ea1b8fbf9768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e3decd0f70c40038db4276617f9e6bc",
            "placeholder": "​",
            "style": "IPY_MODEL_1913c311d6c34db4affb2b1599250704",
            "value": " 19/50 [39:51&lt;2:17:24, 265.96s/it]"
          }
        },
        "c0444c2bd2c64f1587151318d3461127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6d0d43bb6664f129209f3e29ca9dd84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63c13b3ae53f4cd7bb90bd276f99dfb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8f5200ec5a04c7ea7223061b8e1e8ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b45de894d1634977b1bdd0cd3a0aa856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e3decd0f70c40038db4276617f9e6bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1913c311d6c34db4affb2b1599250704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Creatur3245/.isort.cfg/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sOW4dpNnIjkt"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook-style execution for Kaggle-compatible EDA + Baseline + XGBoost (with fallbacks)\n",
        "# This will run in the visible python environment and display outputs for the user.\n",
        "# It attempts to load data from /kaggle/input; if not found, it creates a synthetic dataset\n",
        "# with similar size and structure so the rest of the pipeline can be demonstrated.\n",
        "# Plots use matplotlib (no seaborn) as required by environment rules.\n",
        "# The final outputs (models, summary CSV) will be saved to /mnt/data and links provided.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Listing files under /kaggle/input (if any):\")\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Load the competition data files\n",
        "try:\n",
        "    train_df = pd.read_csv('/kaggle/input/melting-point/train.csv')\n",
        "    test_df = pd.read_csv('/kaggle/input/melting-point/test.csv')\n",
        "    sample_submission_df = pd.read_csv('/kaggle/input/melting-point/sample_submission.csv')\n",
        "    print(\"\\nCompetition data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nCompetition data not found in /kaggle/input. Generating a synthetic dataset for demonstration.\")\n",
        "    rng = np.random.default_rng(42)\n",
        "    n_total = 3328\n",
        "    n_features = 50\n",
        "    # Create synthetic continuous features and a target Tm with non-linear interactions\n",
        "    X = rng.normal(size=(n_total, n_features))\n",
        "    # Create non-linear target with groups to mimic chemical families\n",
        "    families = rng.integers(0, 10, size=n_total)  # 10 families\n",
        "    coef = rng.normal(scale=2.0, size=n_features)\n",
        "    Tm = (X @ coef) + (families * 3.5) + rng.normal(scale=5.0, size=n_total)\n",
        "    train_df = pd.DataFrame(X, columns=[f\"feat_{i}\" for i in range(n_features)])\n",
        "    train_df[\"family\"] = families\n",
        "    train_df[\"Tm\"] = Tm\n",
        "    # Create a synthetic test set and sample submission\n",
        "    X_test_synth = rng.normal(size=(int(n_total*0.2), n_features))\n",
        "    test_df = pd.DataFrame(X_test_synth, columns=[f\"feat_{i}\" for i in range(n_features)])\n",
        "    test_df[\"family\"] = rng.integers(0, 10, size=int(n_total*0.2))\n",
        "    sample_submission_df = pd.DataFrame({'id': range(len(test_df)), 'Tm': np.zeros(len(test_df))})\n",
        "    print(\"Synthetic dataset created.\")\n",
        "\n",
        "# Combine train and test for EDA and feature engineering (if needed later)\n",
        "df = pd.concat([train_df.drop('Tm', axis=1), test_df.drop('id', axis=1)], ignore_index=True)\n",
        "df['Tm'] = pd.concat([train_df['Tm'], pd.Series([np.nan]*len(test_df))], ignore_index=True)\n",
        "\n",
        "\n",
        "print(\"\\nTrain Dataframe head:\")\n",
        "display(train_df.head())\n",
        "print(\"\\nTest Dataframe head:\")\n",
        "display(test_df.head())\n",
        "print(\"\\nSample Submission head:\")\n",
        "display(sample_submission_df.head())\n",
        "\n",
        "print(\"\\nBasic info (Train):\")\n",
        "print(train_df.info())\n",
        "print(\"\\nBasic info (Test):\")\n",
        "print(test_df.info())\n",
        "\n",
        "\n",
        "# Quick EDA numbers (using train_df)\n",
        "n_rows_train, n_cols_train = train_df.shape\n",
        "n_rows_test, n_cols_test = test_df.shape\n",
        "print(f\"\\nTrain Rows: {n_rows_train}, Train Columns: {n_cols_train}\")\n",
        "print(f\"Test Rows: {n_rows_test}, Test Columns: {n_cols_test}\")\n",
        "\n",
        "\n",
        "print(\"\\nMissing values per column (Train - top 10):\")\n",
        "print(train_df.isna().sum().sort_values(ascending=False).head(10))\n",
        "\n",
        "# Target distribution plot (using train_df)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(train_df[\"Tm\"].values, bins=40)\n",
        "plt.title(\"Distribution of Tm (melting point) — Madame Ms Strange says: 'Observe the spread'\")\n",
        "plt.xlabel(\"Tm\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation with target (top 12 - using train_df)\n",
        "corrs = train_df.corr()[\"Tm\"].abs().sort_values(ascending=False)\n",
        "print(\"\\nTop correlations with Tm (Train):\")\n",
        "display(corrs.head(12))\n",
        "\n",
        "# Scatter of top correlated feature vs Tm (using train_df)\n",
        "# Find the top correlated feature excluding Tm itself\n",
        "top_feat = corrs.drop('Tm', errors='ignore').index[0]\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(train_df[top_feat], train_df[\"Tm\"], s=6)\n",
        "plt.title(f\"Scatter: {top_feat} vs Tm (Train)\")\n",
        "plt.xlabel(top_feat)\n",
        "plt.ylabel(\"Tm\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# PCA 2D projection of features (excluding Tm, id & non-numeric family if present - using train_df)\n",
        "feature_cols = [c for c in train_df.columns if c not in (\"Tm\", \"family\", \"id\")]\n",
        "X_eda = train_df[feature_cols].values\n",
        "scaler_eda = StandardScaler()\n",
        "Xs_eda = scaler_eda.fit_transform(X_eda)\n",
        "\n",
        "pca_eda = PCA(n_components=2)\n",
        "proj_eda = pca_eda.fit_transform(Xs_eda)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(proj_eda[:,0], proj_eda[:,1], c=train_df.get(\"family\", pd.Series([0]*len(train_df))), s=8)\n",
        "plt.title(\"PCA (2 components) of features — families colored (Train)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Prepare data for modeling\n",
        "X_train = train_df[feature_cols].values\n",
        "y_train = train_df[\"Tm\"].values\n",
        "X_test = test_df[feature_cols].values # Use feature_cols from train for test set as well\n",
        "\n",
        "print(f\"\\nTrain size for modeling: {X_train.shape[0]}, Test size for prediction: {X_test.shape[0]}\")\n",
        "\n",
        "# Baseline linear model (LinearRegression)\n",
        "lr = make_pipeline(StandardScaler(), LinearRegression())\n",
        "lr.fit(X_train, y_train)\n",
        "pred_lr_train = lr.predict(X_test) # Predict on the actual test set\n",
        "# Note: MAE on test set requires true values, which are not available in competition test data.\n",
        "# We will evaluate on train/validation splits within the training process or predict on the competition test set for submission.\n",
        "# For demonstration, let's predict on the training set to show MAE.\n",
        "pred_lr_train_eval = lr.predict(X_train)\n",
        "mae_lr_train = mean_absolute_error(y_train, pred_lr_train_eval)\n",
        "print(f\"\\nBaseline LinearRegression MAE (on Train): {mae_lr_train:.4f}\")\n",
        "\n",
        "\n",
        "# Ridge regression (regularized linear)\n",
        "ridge = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n",
        "ridge.fit(X_train, y_train)\n",
        "pred_ridge_train = ridge.predict(X_test)\n",
        "pred_ridge_train_eval = ridge.predict(X_train)\n",
        "mae_ridge_train = mean_absolute_error(y_train, pred_ridge_train_eval)\n",
        "print(f\"Ridge Regression MAE (on Train): {mae_ridge_train:.4f}\")\n",
        "\n",
        "\n",
        "# SVR (nonlinear baseline) - may be slower; use small sample if too slow\n",
        "svr = make_pipeline(StandardScaler(), SVR(kernel='rbf', C=1.0, epsilon=0.5))\n",
        "try:\n",
        "    # SVR training can be slow, use a subset if needed\n",
        "    if len(X_train) > 5000: # Example threshold\n",
        "         subset_idx = np.random.choice(len(X_train), 5000, replace=False)\n",
        "         X_train_subset = X_train[subset_idx]\n",
        "         y_train_subset = y_train[subset_idx]\n",
        "         svr.fit(X_train_subset, y_train_subset)\n",
        "         pred_svr_train_eval = svr.predict(X_train[subset_idx])\n",
        "         mae_svr_train = mean_absolute_error(y_train[subset_idx], pred_svr_train_eval)\n",
        "         print(f\"SVR MAE (on Train Subset): {mae_svr_train:.4f}\")\n",
        "         pred_svr_train = svr.predict(X_test) # Predict on full test set\n",
        "    else:\n",
        "        svr.fit(X_train, y_train)\n",
        "        pred_svr_train = svr.predict(X_test)\n",
        "        pred_svr_train_eval = svr.predict(X_train)\n",
        "        mae_svr_train = mean_absolute_error(y_train, pred_svr_train_eval)\n",
        "        print(f\"SVR MAE (on Train): {mae_svr_train:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"SVR failed or was too slow in this environment:\", e)\n",
        "    pred_svr_train = np.zeros(len(X_test)) # Placeholder\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "pred_rf_test = rf.predict(X_test) # Predict on the actual test set\n",
        "pred_rf_train_eval = rf.predict(X_train)\n",
        "mae_rf_train = mean_absolute_error(y_train, pred_rf_train_eval)\n",
        "print(f\"RandomForestRegressor MAE (on Train): {mae_rf_train:.4f}\")\n",
        "\n",
        "\n",
        "# Try XGBoost if available\n",
        "use_xgb = False\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    use_xgb = True\n",
        "    xgb_model = xgb.XGBRegressor(n_estimators=300, random_state=42, verbosity=0)\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    pred_xgb_test = xgb_model.predict(X_test) # Predict on the actual test set\n",
        "    pred_xgb_train_eval = xgb_model.predict(X_train)\n",
        "    mae_xgb_train = mean_absolute_error(y_train, pred_xgb_train_eval)\n",
        "    print(f\"XGBoost MAE (on Train): {mae_xgb_train:.4f}\")\n",
        "except Exception as e:\n",
        "    print(\"XGBoost not available or failed to run here. Skipping XGBoost. Error:\", e)\n",
        "    use_xgb = False\n",
        "    pred_xgb_test = np.zeros(len(X_test)) # Placeholder\n",
        "\n",
        "\n",
        "# Feature importances from RandomForest (top 12)\n",
        "importances = rf.feature_importances_\n",
        "imp_idx = np.argsort(importances)[::-1][:12]\n",
        "top_features = [(feature_cols[i], importances[i]) for i in imp_idx]\n",
        "print(\"\\nTop feature importances (from RandomForest):\")\n",
        "for name, imp in top_features:\n",
        "    print(f\"{name}: {imp:.4f}\")\n",
        "\n",
        "# Save trained models to /mnt/data for Kaggle output download\n",
        "os.makedirs('/mnt/data/models', exist_ok=True)\n",
        "joblib.dump(lr, '/mnt/data/models/linear_baseline.joblib')\n",
        "joblib.dump(ridge, '/mnt/data/models/ridge_baseline.joblib')\n",
        "joblib.dump(rf, '/mnt/data/models/rf_baseline.joblib')\n",
        "if use_xgb:\n",
        "    joblib.dump(xgb_model, '/mnt/data/models/xgb_baseline.joblib')\n",
        "\n",
        "# Produce submission CSV - This will be overwritten by the tuning cell later\n",
        "# submission_df = sample_submission_df.copy()\n",
        "# submission_df['Tm'] = pred_rf_test # Using baseline RF predictions initially\n",
        "# submission_df.to_csv('/mnt/data/submission.csv', index=False)\n",
        "\n",
        "\n",
        "print(\"\\nSaved models to /mnt/data/models.\")\n",
        "print(\"Files saved:\")\n",
        "for dirname, _, filenames in os.walk('/mnt/data'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Final small diagnostic plot (Actual vs Predicted on TRAIN set for best model among RF and XGB if available)\n",
        "# We plot against the training set as we don't have true labels for the competition test set.\n",
        "best_pred_eval = pred_rf_train_eval\n",
        "best_name = \"RandomForest\"\n",
        "best_mae_eval = mae_rf_train\n",
        "if use_xgb and mae_xgb_train < best_mae_eval:\n",
        "    best_pred_eval = pred_xgb_train_eval\n",
        "    best_name = \"XGBoost\"\n",
        "    best_mae_eval = mae_xgb_train\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(y_train, best_pred_eval, s=8)\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=2)\n",
        "plt.title(f\"Actual vs Predicted — {best_name} (MAE={best_mae_eval:.3f}) on Train Set\")\n",
        "plt.xlabel(\"Actual Tm (Train)\")\n",
        "plt.ylabel(\"Predicted Tm (Train)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cross-validated MAE (K-Fold) for RandomForest as a robust estimate (on Train set)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# Check if 'family' column exists and has multiple unique values for GroupKFold\n",
        "if 'family' in train_df.columns and len(train_df['family'].unique()) > 1:\n",
        "     cv_strategy = GroupKFold(n_splits=5)\n",
        "     groups_train = train_df['family']\n",
        "     print(\"\\nUsing GroupKFold for cross-validation.\")\n",
        "else:\n",
        "     cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "     groups_train = None\n",
        "     print(\"\\nUsing KFold for cross-validation.\")\n",
        "\n",
        "\n",
        "cv_scores = -cross_val_score(RandomForestRegressor(n_estimators=150, random_state=42, n_jobs=-1),\n",
        "                            X_train, y_train, scoring='neg_mean_absolute_error', cv=cv_strategy, groups=groups_train, n_jobs=-1)\n",
        "print(f\"\\n5-fold CV RandomForest MAE (on Train): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "# Save a brief run summary text\n",
        "summary_text = f\"\"\"\n",
        "Run summary — Madame Ms Strange oracle voice:\n",
        "Train Rows: {n_rows_train}, Test Rows: {n_rows_test}, Features: {len(feature_cols)}.\n",
        "Baseline Linear MAE (Train): {mae_lr_train:.4f}\n",
        "Ridge MAE (Train): {mae_ridge_train:.4f}\n",
        "RandomForest MAE (Train): {mae_rf_train:.4f}\n",
        "{'XGBoost MAE (Train): {:.4f}'.format(mae_xgb_train) if use_xgb else 'XGBoost: not available in this environment.'}\n",
        "\n",
        "Artifacts saved to /mnt/data/models and /mnt/data/submission.csv (generated by tuning cell)\n",
        "\"\"\"\n",
        "with open('/mnt/data/run_summary.txt', 'w') as f:\n",
        "    f.write(summary_text)\n",
        "\n",
        "print(\"\\nRun summary saved to /mnt/data/run_summary.txt\")\n",
        "print(\"\\nNotebook execution complete. Look for outputs in /mnt/data/.\")"
      ],
      "metadata": {
        "id": "M--XvkaGInD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GroupKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import joblib, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Load dataset from /kaggle/input ---\n",
        "try:\n",
        "    train_df = pd.read_csv('/kaggle/input/melting-point/train.csv')\n",
        "    test_df = pd.read_csv('/kaggle/input/melting-point/test.csv')\n",
        "    sample_submission_df = pd.read_csv('/kaggle/input/melting-point/sample_submission.csv')\n",
        "    print(\"Competition data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Competition data not found in /kaggle/input. Using synthetic dataset if available from previous cell.\")\n",
        "    # Assuming synthetic data was generated and 'df' exists from a previous cell\n",
        "    if 'df' not in locals():\n",
        "         print(\"No competition data and no synthetic data ('df') found. Please run the previous cell.\")\n",
        "         # Exit or handle this case appropriately\n",
        "    else:\n",
        "         # If synthetic data 'df' exists, split it into train/test for tuning\n",
        "         # This split will be different from the main EDA cell's split, but serves for tuning demonstration\n",
        "         rng = np.random.default_rng(42)\n",
        "         n_total = len(df)\n",
        "         n_train = len(df) - len(test_df) # Assuming test_df was created synthetic with the rest of df\n",
        "         train_df = df.iloc[:n_train].copy()\n",
        "         test_df = df.iloc[n_train:].copy()\n",
        "         test_df['id'] = range(len(test_df)) # Add dummy id for synthetic test\n",
        "         sample_submission_df = pd.DataFrame({'id': test_df['id'], 'Tm': np.zeros(len(test_df))})\n",
        "         print(\"Using synthetic data split for tuning.\")\n",
        "\n",
        "\n",
        "# --- Assume 'Tm' is the target ---\n",
        "target = \"Tm\"\n",
        "assert target in train_df.columns, f\"Expected column '{target}' not found in training dataset!\"\n",
        "feature_cols = [c for c in train_df.columns if c != target and train_df[c].dtype != 'object' and c != 'id']\n",
        "\n",
        "X_train = train_df[feature_cols].values\n",
        "y_train = train_df[target].values\n",
        "X_test = test_df[feature_cols].values # Use feature_cols from train for test set\n",
        "\n",
        "# Determine groups for GroupKFold if 'family' exists and has multiple unique values in train_df\n",
        "if 'family' in train_df.columns and len(train_df['family'].unique()) > 1:\n",
        "    groups_train = train_df['family'].values\n",
        "    cv = GroupKFold(n_splits=5)\n",
        "    print(\"Using GroupKFold for cross-validation.\")\n",
        "else:\n",
        "    groups_train = None\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    print(\"Using KFold for cross-validation.\")\n",
        "\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "# --- RandomForest tuning ---\n",
        "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "param_dist_rf = {\n",
        "    \"n_estimators\": [100, 300, 500, 800],\n",
        "    \"max_depth\": [None, 10, 20, 40],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "}\n",
        "search_rf = RandomizedSearchCV(\n",
        "    rf, param_distributions=param_dist_rf,\n",
        "    n_iter=25, scoring=mae_scorer, cv=cv, n_jobs=-1, random_state=42, verbose=1\n",
        ")\n",
        "\n",
        "# Fit with groups if GroupKFold is used\n",
        "if isinstance(cv, GroupKFold):\n",
        "    search_rf.fit(X_train, y_train, groups=groups_train)\n",
        "else:\n",
        "    search_rf.fit(X_train, y_train)\n",
        "\n",
        "best_rf = search_rf.best_estimator_\n",
        "print(\"Best RF params:\", search_rf.best_params_)\n",
        "print(\"Best RF CV score (MAE):\", -search_rf.best_score_)\n",
        "\n",
        "# --- Evaluate on train set (since test set true labels are not available) ---\n",
        "pred_rf_train_eval = best_rf.predict(X_train)\n",
        "mae_rf_train = mean_absolute_error(y_train, pred_rf_train_eval)\n",
        "print(\"Tuned RF Train MAE:\", mae_rf_train)\n",
        "\n",
        "# Predict on the actual competition test set\n",
        "pred_rf_test = best_rf.predict(X_test)\n",
        "\n",
        "\n",
        "# --- Save model ---\n",
        "os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n",
        "joblib.dump(best_rf, \"/kaggle/working/models/rf_tuned.joblib\")\n",
        "\n",
        "# --- Try XGBoost if available ---\n",
        "use_xgb = False\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    use_xgb = True\n",
        "    xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1, tree_method='hist') # Use hist for potentially faster training\n",
        "    param_dist_xgb = {\n",
        "        \"n_estimators\": [200, 500, 800],\n",
        "        \"max_depth\": [3, 6, 10],\n",
        "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "        \"subsample\": [0.7, 0.9, 1.0],\n",
        "        \"colsample_bytree\": [0.7, 0.9, 1.0],\n",
        "    }\n",
        "    search_xgb = RandomizedSearchCV(\n",
        "        xgb_model, param_distributions=param_dist_xgb,\n",
        "        n_iter=25, scoring=mae_scorer, cv=cv, n_jobs=-1, random_state=42, verbose=1\n",
        "    )\n",
        "    # Fit with groups if GroupKFold is used\n",
        "    if isinstance(cv, GroupKFold):\n",
        "        search_xgb.fit(X_train, y_train, groups=groups_train)\n",
        "    else:\n",
        "        search_xgb.fit(X_train, y_train)\n",
        "\n",
        "    best_xgb = search_xgb.best_estimator_\n",
        "    print(\"Best XGB params:\", search_xgb.best_params_)\n",
        "    print(\"Best XGB CV score (MAE):\", -search_xgb.best_score_)\n",
        "\n",
        "    # Evaluate on train set\n",
        "    pred_xgb_train_eval = best_xgb.predict(X_train)\n",
        "    mae_xgb_train = mean_absolute_error(y_train, pred_xgb_train_eval)\n",
        "    print(\"Tuned XGB Train MAE:\", mae_xgb_train)\n",
        "\n",
        "    # Predict on the actual competition test set\n",
        "    pred_xgb_test = best_xgb.predict(X_test)\n",
        "\n",
        "    joblib.dump(best_xgb, \"/kaggle/working/models/xgb_tuned.joblib\")\n",
        "except Exception as e:\n",
        "    print(\"XGBoost tuning skipped:\", e)\n",
        "    use_xgb = False # Ensure use_xgb is False if import fails\n",
        "    pred_xgb_test = np.zeros(len(X_test)) # Placeholder\n",
        "\n",
        "\n",
        "# --- Generate sample submission file ---\n",
        "# Using the best model's predictions on the test set (based on CV score)\n",
        "best_model_name = \"RandomForest\"\n",
        "best_preds = pred_rf_test\n",
        "best_cv_mae = -search_rf.best_score_\n",
        "\n",
        "if use_xgb and (-search_xgb.best_score_ < best_cv_mae):\n",
        "    best_model_name = \"XGBoost\"\n",
        "    best_preds = pred_xgb_test\n",
        "    best_cv_mae = -search_xgb.best_score_\n",
        "\n",
        "submission_df = sample_submission_df.copy()\n",
        "submission_df['Tm'] = best_preds # Use predictions on the competition test set\n",
        "submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
        "\n",
        "\n",
        "print(f\"\\nSaved tuned models to /kaggle/working/models and a sample submission.csv (based on {best_model_name}) to /kaggle/working.\")\n",
        "print(\"Files saved in /kaggle/working:\")\n",
        "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS3ha29YInez",
        "outputId": "60735749-04b4-4b58-927d-02c2216fd58f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CSV found under /kaggle/input — generating a synthetic dataset for demonstration.\n",
            "Synthetic dataset created.\n",
            "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
            "Best RF params: {'n_estimators': 800, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': None}\n",
            "Best RF CV score (MAE): 10.505212037988656\n",
            "Tuned RF Test MAE: 9.62203938266791\n",
            "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
            "Best XGB params: {'subsample': 0.7, 'n_estimators': 800, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n",
            "Best XGB CV score (MAE): 6.70125396186482\n",
            "Tuned XGB Test MAE: 5.497477001896453\n",
            "\n",
            "Saved tuned models to /kaggle/working/models and a sample submission.csv (based on XGBoost) to /kaggle/working.\n",
            "Files saved in /kaggle/working:\n",
            "/kaggle/working/submission.csv\n",
            "/kaggle/working/models/rf_tuned.joblib\n",
            "/kaggle/working/models/xgb_tuned.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip kaggle competitions download -c melting-point"
      ],
      "metadata": {
        "id": "rqgHgFy8O4b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145cbbff-d38b-4b4c-8d52-727652463a1f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"kaggle\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna --quiet\n",
        "\n",
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score, GroupKFold, KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer\n",
        "import xgboost as xgb\n",
        "\n",
        "# --- CV setup ---\n",
        "if groups is not None:\n",
        "    cv = GroupKFold(n_splits=5)\n",
        "    group_train = train_df['family']\n",
        "else:\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    group_train = None\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "# ---------------------------\n",
        "# RandomForest Optuna Tuning\n",
        "# ---------------------------\n",
        "def objective_rf(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 50),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
        "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "    model = RandomForestRegressor(**params)\n",
        "    if groups is not None:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=mae_scorer, groups=group_train, n_jobs=-1)\n",
        "    else:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=mae_scorer, n_jobs=-1)\n",
        "    return -scores.mean()\n",
        "\n",
        "study_rf = optuna.create_study(direction=\"minimize\")\n",
        "study_rf.optimize(objective_rf, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"Best RF Params:\", study_rf.best_params)\n",
        "print(\"Best RF CV MAE:\", study_rf.best_value)\n",
        "\n",
        "best_rf = RandomForestRegressor(**study_rf.best_params, random_state=42, n_jobs=-1)\n",
        "best_rf.fit(X_train, y_train)\n",
        "pred_rf = best_rf.predict(X_test)\n",
        "mae_rf = mean_absolute_error(y_test, pred_rf)\n",
        "print(\"Optuna RF Test MAE:\", mae_rf)\n",
        "\n",
        "joblib.dump(best_rf, \"/kaggle/working/models/rf_optuna.joblib\")\n",
        "\n",
        "# ---------------------------\n",
        "# XGBoost Optuna Tuning\n",
        "# ---------------------------\n",
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "    if groups is not None:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=mae_scorer, groups=group_train, n_jobs=-1)\n",
        "    else:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=mae_scorer, n_jobs=-1)\n",
        "    return -scores.mean()\n",
        "\n",
        "study_xgb = optuna.create_study(direction=\"minimize\")\n",
        "study_xgb.optimize(objective_xgb, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"Best XGB Params:\", study_xgb.best_params)\n",
        "print(\"Best XGB CV MAE:\", study_xgb.best_value)\n",
        "\n",
        "best_xgb = xgb.XGBRegressor(**study_xgb.best_params, random_state=42, n_jobs=-1)\n",
        "best_xgb.fit(X_train, y_train)\n",
        "pred_xgb = best_xgb.predict(X_test)\n",
        "mae_xgb = mean_absolute_error(y_test, pred_xgb)\n",
        "print(\"Optuna XGB Test MAE:\", mae_xgb)\n",
        "\n",
        "joblib.dump(best_xgb, \"/kaggle/working/models/xgb_optuna.joblib\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "87bbe96e70854624837ddf009990f3ee",
            "c7511ee490724b10aca8329cb1d7656c",
            "f892209085ad433482e044155e8d85ce",
            "83fe12f693a84312a335ea1b8fbf9768",
            "c0444c2bd2c64f1587151318d3461127",
            "b6d0d43bb6664f129209f3e29ca9dd84",
            "63c13b3ae53f4cd7bb90bd276f99dfb0",
            "f8f5200ec5a04c7ea7223061b8e1e8ab",
            "b45de894d1634977b1bdd0cd3a0aa856",
            "7e3decd0f70c40038db4276617f9e6bc",
            "1913c311d6c34db4affb2b1599250704"
          ]
        },
        "id": "n6mjDJxgJuTk",
        "outputId": "ec7503e4-41fd-4976-a118-e1ca010e313b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-02 20:05:57,257] A new study created in memory with name: no-name-baef1c14-84a8-495a-9f74-f69695462d10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87bbe96e70854624837ddf009990f3ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I 2025-10-02 20:07:01,974] Trial 0 finished with value: 10.632652465507961 and parameters: {'n_estimators': 943, 'max_depth': 22, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 0 with value: 10.632652465507961.\n",
            "[I 2025-10-02 20:07:43,665] Trial 1 finished with value: 11.081403315870672 and parameters: {'n_estimators': 862, 'max_depth': 18, 'min_samples_split': 12, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 0 with value: 10.632652465507961.\n",
            "[I 2025-10-02 20:07:55,810] Trial 2 finished with value: 10.781309932316478 and parameters: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 0 with value: 10.632652465507961.\n",
            "[I 2025-10-02 20:08:02,625] Trial 3 finished with value: 10.950743424502171 and parameters: {'n_estimators': 112, 'max_depth': 10, 'min_samples_split': 12, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 0 with value: 10.632652465507961.\n",
            "[I 2025-10-02 20:08:13,520] Trial 4 finished with value: 10.644038868486279 and parameters: {'n_estimators': 169, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 0 with value: 10.632652465507961.\n",
            "[I 2025-10-02 20:08:22,905] Trial 5 finished with value: 12.134304805764918 and parameters: {'n_estimators': 303, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 0 with value: 10.632652465507961.\n",
            "[I 2025-10-02 20:08:52,646] Trial 6 finished with value: 10.853272894504885 and parameters: {'n_estimators': 485, 'max_depth': 41, 'min_samples_split': 20, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 0 with value: 10.632652465507961.\n",
            "[I 2025-10-02 20:11:33,357] Trial 7 finished with value: 10.584320362164473 and parameters: {'n_estimators': 451, 'max_depth': 37, 'min_samples_split': 12, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 7 with value: 10.584320362164473.\n",
            "[I 2025-10-02 20:12:07,213] Trial 8 finished with value: 11.03571704250508 and parameters: {'n_estimators': 613, 'max_depth': 9, 'min_samples_split': 18, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 7 with value: 10.584320362164473.\n",
            "[I 2025-10-02 20:12:32,435] Trial 9 finished with value: 10.542136291538023 and parameters: {'n_estimators': 315, 'max_depth': 42, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 9 with value: 10.542136291538023.\n",
            "[I 2025-10-02 20:16:15,319] Trial 10 finished with value: 10.630675919029745 and parameters: {'n_estimators': 691, 'max_depth': 50, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 9 with value: 10.542136291538023.\n",
            "[I 2025-10-02 20:18:30,602] Trial 11 finished with value: 10.6157505140927 and parameters: {'n_estimators': 410, 'max_depth': 37, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 9 with value: 10.542136291538023.\n",
            "[I 2025-10-02 20:20:20,566] Trial 12 finished with value: 10.744873753592318 and parameters: {'n_estimators': 377, 'max_depth': 34, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': None}. Best is trial 9 with value: 10.542136291538023.\n",
            "[I 2025-10-02 20:23:36,910] Trial 13 finished with value: 10.561845810070453 and parameters: {'n_estimators': 562, 'max_depth': 46, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 9 with value: 10.542136291538023.\n",
            "[I 2025-10-02 20:28:39,081] Trial 14 finished with value: 10.526602478833187 and parameters: {'n_estimators': 683, 'max_depth': 48, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 14 with value: 10.526602478833187.\n",
            "[I 2025-10-02 20:29:26,003] Trial 15 finished with value: 10.91735639836811 and parameters: {'n_estimators': 756, 'max_depth': 29, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 14 with value: 10.526602478833187.\n",
            "[I 2025-10-02 20:34:48,840] Trial 16 finished with value: 10.524014841430864 and parameters: {'n_estimators': 731, 'max_depth': 44, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 16 with value: 10.524014841430864.\n",
            "[I 2025-10-02 20:40:11,798] Trial 17 finished with value: 10.521647034198045 and parameters: {'n_estimators': 784, 'max_depth': 49, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 17 with value: 10.521647034198045.\n",
            "[I 2025-10-02 20:45:48,481] Trial 18 finished with value: 10.522361606045541 and parameters: {'n_estimators': 824, 'max_depth': 43, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 17 with value: 10.521647034198045.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna --quiet\n",
        "\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from sklearn.model_selection import cross_val_score, GroupKFold, KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "\n",
        "# --- CV setup ---\n",
        "if groups is not None:\n",
        "    cv = GroupKFold(n_splits=5)\n",
        "    group_train = train_df['family']\n",
        "else:\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    group_train = None\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "# ---------------------------\n",
        "# RandomForest with Optuna + Pruning\n",
        "# ---------------------------\n",
        "def objective_rf(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 50),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
        "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "    model = RandomForestRegressor(**params)\n",
        "    if groups is not None:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=mae_scorer, groups=group_train, n_jobs=-1)\n",
        "    else:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=mae_scorer, n_jobs=-1)\n",
        "    score = -scores.mean()\n",
        "\n",
        "    # Report to Optuna and prune if bad\n",
        "    trial.report(score, step=0)\n",
        "    if trial.should_prune():\n",
        "        raise optuna.TrialPruned()\n",
        "    return score\n",
        "\n",
        "study_rf = optuna.create_study(direction=\"minimize\", pruner=MedianPruner())\n",
        "study_rf.optimize(objective_rf, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"Best RF Params:\", study_rf.best_params)\n",
        "print(\"Best RF CV MAE:\", study_rf.best_value)\n",
        "\n",
        "best_rf = RandomForestRegressor(**study_rf.best_params, random_state=42, n_jobs=-1)\n",
        "best_rf.fit(X_train, y_train)\n",
        "pred_rf = best_rf.predict(X_test)\n",
        "mae_rf = mean_absolute_error(y_test, pred_rf)\n",
        "print(\"Optuna RF Test MAE:\", mae_rf)\n",
        "\n",
        "joblib.dump(best_rf, \"/kaggle/working/models/rf_optuna_pruned.joblib\")\n",
        "\n",
        "# ---------------------------\n",
        "# XGBoost with Optuna + Pruning\n",
        "# ---------------------------\n",
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "    model = xgb.XGBRegressor(**params, tree_method=\"hist\", eval_metric=\"mae\")\n",
        "\n",
        "    # Optuna pruning via callbacks (XGBoost integration)\n",
        "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation_0-mae\")\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "        verbose=False,\n",
        "        callbacks=[pruning_callback]\n",
        "    )\n",
        "    pred = model.predict(X_valid)\n",
        "    score = mean_absolute_error(y_valid, pred)\n",
        "    return score\n",
        "\n",
        "# Split train into train/valid for pruning\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_sub, X_valid, y_train_sub, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "study_xgb = optuna.create_study(direction=\"minimize\", pruner=MedianPruner())\n",
        "study_xgb.optimize(objective_xgb, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"Best XGB Params:\", study_xgb.best_params)\n",
        "print(\"Best XGB CV MAE:\", study_xgb.best_value)\n",
        "\n",
        "best_xgb = xgb.XGBRegressor(**study_xgb.best_params, random_state=42, n_jobs=-1, tree_method=\"hist\")\n",
        "best_xgb.fit(X_train, y_train)\n",
        "pred_xgb = best_xgb.predict(X_test)\n",
        "mae_xgb = mean_absolute_error(y_test, pred_xgb)\n",
        "print(\"Optuna XGB Test MAE:\", mae_xgb)\n",
        "\n",
        "joblib.dump(best_xgb, \"/kaggle/working/models/xgb_optuna_pruned.joblib\")\n"
      ],
      "metadata": {
        "id": "mdDCuGaZJ_T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_parallel_coordinate\n",
        "from optuna.visualization import plot_contour, plot_slice\n",
        "\n",
        "# ---------------------------\n",
        "# RandomForest Optuna Plots\n",
        "# ---------------------------\n",
        "print(\"📊 RandomForest Study Visualization\")\n",
        "\n",
        "fig1 = plot_optimization_history(study_rf)\n",
        "fig1.show()\n",
        "\n",
        "fig2 = plot_param_importances(study_rf)\n",
        "fig2.show()\n",
        "\n",
        "fig3 = plot_parallel_coordinate(study_rf)\n",
        "fig3.show()\n",
        "\n",
        "fig4 = plot_contour(study_rf)\n",
        "fig4.show()\n",
        "\n",
        "fig5 = plot_slice(study_rf)\n",
        "fig5.show()\n",
        "\n",
        "# ---------------------------\n",
        "# XGBoost Optuna Plots\n",
        "# ---------------------------\n",
        "print(\"📊 XGBoost Study Visualization\")\n",
        "\n",
        "fig6 = plot_optimization_history(study_xgb)\n",
        "fig6.show()\n",
        "\n",
        "fig7 = plot_param_importances(study_xgb)\n",
        "fig7.show()\n",
        "\n",
        "fig8 = plot_parallel_coordinate(study_xgb)\n",
        "fig8.show()\n",
        "\n",
        "fig9 = plot_contour(study_xgb)\n",
        "fig9.show()\n",
        "\n",
        "fig10 = plot_slice(study_xgb)\n",
        "fig10.show()\n"
      ],
      "metadata": {
        "id": "_C-9Sf-tJual"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# Compare MAE (RF vs XGB)\n",
        "# ---------------------------\n",
        "model_names = [\"RandomForest\", \"XGBoost\"]\n",
        "mae_scores = [mae_rf, mae_xgb]\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=model_names, y=mae_scores, palette=\"viridis\")\n",
        "plt.title(\"🔮 Madame Ms Strange Showdown: RF vs XGB\", fontsize=14)\n",
        "plt.ylabel(\"Mean Absolute Error (MAE)\")\n",
        "for i, score in enumerate(mae_scores):\n",
        "    plt.text(i, score+0.2, f\"{score:.3f}\", ha='center', fontsize=12, fontweight=\"bold\")\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Residual Distribution\n",
        "# ---------------------------\n",
        "residuals_rf = y_test - pred_rf\n",
        "residuals_xgb = y_test - pred_xgb\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(residuals_rf, shade=True, label=\"RandomForest Residuals\", color=\"darkblue\")\n",
        "sns.kdeplot(residuals_xgb, shade=True, label=\"XGBoost Residuals\", color=\"darkgreen\")\n",
        "plt.axvline(0, color=\"black\", linestyle=\"--\")\n",
        "plt.title(\"Residual Distribution 🌌 (RF vs XGB)\", fontsize=14)\n",
        "plt.xlabel(\"Prediction Error\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Scatter: True vs Predicted\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(y_test, pred_rf, alpha=0.4, label=\"RF Predictions\", color=\"navy\")\n",
        "plt.scatter(y_test, pred_xgb, alpha=0.4, label=\"XGB Predictions\", color=\"teal\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
        "plt.title(\"True vs Predicted 🔮 (RF vs XGB)\", fontsize=14)\n",
        "plt.xlabel(\"True Melting Point (Tm)\")\n",
        "plt.ylabel(\"Predicted Tm\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Print final cosmic verdict\n",
        "# ---------------------------\n",
        "print(\"⚖️ Cosmic Model Verdict\")\n",
        "print(f\"RandomForest Optuna MAE: {mae_rf:.4f}\")\n",
        "print(f\"XGBoost Optuna MAE: {mae_xgb:.4f}\")\n",
        "\n",
        "if mae_rf < mae_xgb:\n",
        "    print(\"✨ RandomForest holds the throne in this dimension.\")\n",
        "else:\n",
        "    print(\"✨ XGBoost reigns supreme across the melting multiverse.\")\n"
      ],
      "metadata": {
        "id": "cnaTesN4Judd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap --quiet\n",
        "import shap\n",
        "\n",
        "# Use a subset of test data for faster SHAP\n",
        "X_sample = X_test[:200]\n",
        "\n",
        "# ---------------------------\n",
        "# SHAP for RandomForest\n",
        "# ---------------------------\n",
        "explainer_rf = shap.TreeExplainer(best_rf)\n",
        "shap_values_rf = explainer_rf.shap_values(X_sample)\n",
        "\n",
        "plt.title(\"🔮 SHAP Summary: RandomForest\")\n",
        "shap.summary_plot(shap_values_rf, X_sample, plot_type=\"bar\", show=True)\n",
        "\n",
        "shap.summary_plot(shap_values_rf, X_sample, show=True)\n",
        "\n",
        "# ---------------------------\n",
        "# SHAP for XGBoost\n",
        "# ---------------------------\n",
        "explainer_xgb = shap.TreeExplainer(best_xgb)\n",
        "shap_values_xgb = explainer_xgb.shap_values(X_sample)\n",
        "\n",
        "plt.title(\"🔮 SHAP Summary: XGBoost\")\n",
        "shap.summary_plot(shap_values_xgb, X_sample, plot_type=\"bar\", show=True)\n",
        "\n",
        "shap.summary_plot(shap_values_xgb, X_sample, show=True)\n",
        "\n",
        "# ---------------------------\n",
        "# Force Plot Example (Single Prediction)\n",
        "# ---------------------------\n",
        "idx = 5  # pick one test sample\n",
        "shap.initjs()\n",
        "shap.force_plot(\n",
        "    explainer_xgb.expected_value,\n",
        "    shap_values_xgb[idx,:],\n",
        "    X_sample.iloc[idx,:],\n",
        "    matplotlib=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "jbw0nB1XKbAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 🔮 Madame Ms Strange EDA — Full Cell\n",
        "# This cell contains visualizations and analysis to compare the performance and interpret the predictions of the tuned RandomForest and XGBoost models.\n",
        "# ===========================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming mae_rf, mae_xgb, y_test, pred_rf, pred_xgb, best_rf, best_xgb, X_test are available from previous cells.\n",
        "# If not, ensure those cells are run first.\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Compare MAE (RF vs XGB)\n",
        "# Visualize the Mean Absolute Error of the two models.\n",
        "# ---------------------------\n",
        "model_names = [\"RandomForest\", \"XGBoost\"]\n",
        "mae_scores = [mae_rf, mae_xgb]\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=model_names, y=mae_scores, palette=\"viridis\")\n",
        "plt.title(\"🔮 Madame Ms Strange Showdown: RF vs XGB - Mean Absolute Error Comparison\", fontsize=14)\n",
        "plt.ylabel(\"Mean Absolute Error (MAE)\")\n",
        "plt.xlabel(\"Model\")\n",
        "for i, score in enumerate(mae_scores):\n",
        "    plt.text(i, score+0.01, f\"{score:.3f}\", ha='center', fontsize=12, fontweight=\"bold\") # Adjusted text position slightly\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Residual Distribution\n",
        "# Plot the distribution of prediction errors (residuals).\n",
        "# A good model should have residuals centered around zero.\n",
        "# ---------------------------\n",
        "residuals_rf = y_test - pred_rf\n",
        "residuals_xgb = y_test - pred_xgb\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(residuals_rf, shade=True, label=\"RandomForest Residuals\", color=\"darkblue\")\n",
        "sns.kdeplot(residuals_xgb, shade=True, label=\"XGBoost Residuals\", color=\"darkgreen\")\n",
        "plt.axvline(0, color=\"black\", linestyle=\"--\", label=\"Zero Error\")\n",
        "plt.title(\"Residual Distribution 🌌 (RF vs XGB)\", fontsize=14)\n",
        "plt.xlabel(\"Prediction Error\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Scatter: True vs Predicted\n",
        "# Visualize the relationship between actual and predicted values.\n",
        "# Points should ideally lie along the diagonal line.\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(y_test, pred_rf, alpha=0.4, label=\"RF Predictions\", color=\"navy\", s=15) # Reduced point size\n",
        "plt.scatter(y_test, pred_xgb, alpha=0.4, label=\"XGB Predictions\", color=\"teal\", s=15) # Reduced point size\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label=\"Ideal Prediction\")\n",
        "plt.title(\"True vs Predicted Tm 🔮 (RF vs XGB)\", fontsize=14)\n",
        "plt.xlabel(\"True Melting Point (Tm)\")\n",
        "plt.ylabel(\"Predicted Tm\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6) # Added grid\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# 4. SHAP Interpretability\n",
        "# Use SHAP values to explain the model predictions.\n",
        "# This section visualizes feature importance and impact.\n",
        "# ---------------------------\n",
        "print(\"\\n--- SHAP Analysis ---\")\n",
        "# Subset sample for SHAP for faster computation\n",
        "# Ensure X_test is a pandas DataFrame with feature names for SHAP plots\n",
        "if not isinstance(X_test, pd.DataFrame):\n",
        "    # Assuming feature_cols are available from previous cells\n",
        "    X_test_df = pd.DataFrame(X_test, columns=feature_cols)\n",
        "    X_sample = X_test_df.sample(n=min(200, len(X_test_df)), random_state=42).copy() # Use sample instead of slicing\n",
        "else:\n",
        "    X_sample = X_test.sample(n=min(200, len(X_test)), random_state=42).copy() # Use sample instead of slicing\n",
        "\n",
        "# ---- Grouping features into interpretable families for better visualization\n",
        "# Assuming original feature names are feat_0, feat_1, ... feat_49 and 'family'\n",
        "feature_groups = {\n",
        "    \"Atomic_Descriptors\": [f\"feat_{i}\" for i in range(0,10)],\n",
        "    \"Bonding_Descriptors\": [f\"feat_{i}\" for i in range(10,20)],\n",
        "    \"Electronic_Descriptors\": [f\"feat_{i}\" for i in range(20,30)],\n",
        "    \"Topological_Descriptors\": [f\"feat_{i}\" for i in range(30,40)],\n",
        "    \"Other_Descriptors\": [f\"feat_{i}\" for i in range(40,50)],\n",
        "    \"Family_ID\": [\"family\"] # Include 'family' if it's a feature used in the model\n",
        "}\n",
        "\n",
        "# Create a mapping from original feature names to group names\n",
        "feature_to_group = {}\n",
        "for group, cols in feature_groups.items():\n",
        "    for col in cols:\n",
        "        feature_to_group[col] = group\n",
        "\n",
        "# Rename columns in the sample DataFrame based on the grouping\n",
        "# Handle potential missing 'family' column if it wasn't used as a feature\n",
        "if 'family' in X_sample.columns and 'family' not in feature_to_group:\n",
        "     feature_to_group['family'] = 'Family_ID' # Add 'family' to mapping if present in data but not in initial groups\n",
        "\n",
        "X_sample_grouped_cols = []\n",
        "for col in X_sample.columns:\n",
        "    X_sample_grouped_cols.append(feature_to_group.get(col, col)) # Use original name if not in mapping\n",
        "\n",
        "X_sample_grouped = X_sample.copy()\n",
        "X_sample_grouped.columns = X_sample_grouped_cols\n",
        "\n",
        "\n",
        "# ---- SHAP for RandomForest\n",
        "print(\"\\nCalculating SHAP values for RandomForest...\")\n",
        "try:\n",
        "    explainer_rf = shap.TreeExplainer(best_rf)\n",
        "    shap_values_rf = explainer_rf.shap_values(X_sample) # Calculate SHAP on original feature names\n",
        "    # Map SHAP values back to grouped feature names for plotting\n",
        "    shap_values_rf_grouped = pd.DataFrame(shap_values_rf, columns=X_sample.columns)\n",
        "    shap_values_rf_grouped.columns = X_sample_grouped.columns.tolist()\n",
        "    shap_values_rf_grouped = shap_values_rf_grouped.groupby(level=0, axis=1).sum() # Sum SHAP values for features in the same group\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title(\"🔮 SHAP Summary: RandomForest (Grouped Features - Bar Plot)\", fontsize=14)\n",
        "    shap.summary_plot(shap_values_rf_grouped, X_sample_grouped, plot_type=\"bar\", show=False) # Use show=False to control figure display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.title(\"🔮 SHAP Summary: RandomForest (Grouped Features - Dot Plot)\", fontsize=14)\n",
        "    shap.summary_plot(shap_values_rf_grouped, X_sample_grouped, show=False) # Use show=False\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"SHAP for RandomForest failed: {e}\")\n",
        "\n",
        "\n",
        "# ---- SHAP for XGBoost\n",
        "print(\"\\nCalculating SHAP values for XGBoost...\")\n",
        "try:\n",
        "    explainer_xgb = shap.TreeExplainer(best_xgb)\n",
        "    shap_values_xgb = explainer_xgb.shap_values(X_sample) # Calculate SHAP on original feature names\n",
        "    # Map SHAP values back to grouped feature names for plotting\n",
        "    shap_values_xgb_grouped = pd.DataFrame(shap_values_xgb, columns=X_sample.columns)\n",
        "    shap_values_xgb_grouped.columns = X_sample_grouped.columns.tolist()\n",
        "    shap_values_xgb_grouped = shap_values_xgb_grouped.groupby(level=0, axis=1).sum() # Sum SHAP values for features in the same group\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title(\"🔮 SHAP Summary: XGBoost (Grouped Features - Bar Plot)\", fontsize=14)\n",
        "    shap.summary_plot(shap_values_xgb_grouped, X_sample_grouped, plot_type=\"bar\", show=False) # Use show=False\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.title(\"🔮 SHAP Summary: XGBoost (Grouped Features - Dot Plot)\", fontsize=14)\n",
        "    shap.summary_plot(shap_values_xgb_grouped, X_sample_grouped, show=False) # Use show=False\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"SHAP for XGBoost failed: {e}\")\n",
        "\n",
        "\n",
        "# ---- SHAP Force Plot Example (Single Prediction)\n",
        "# Choose an index for the force plot. Ensure the index is within the bounds of X_sample.\n",
        "if len(X_sample) > 0:\n",
        "    idx = 0 # Use the first sample from the subset\n",
        "    print(f\"\\nGenerating SHAP Force Plot for sample index {idx}...\")\n",
        "    try:\n",
        "        # Need to calculate SHAP values for the specific instance using the original explainer\n",
        "        shap_values_xgb_single = explainer_xgb.shap_values(X_sample.iloc[idx,:])\n",
        "        shap.initjs() # Initialize JS for interactive plot\n",
        "        # Use the original X_sample row for the force plot, but the grouped column names for display\n",
        "        force_plot_data = X_sample.iloc[idx,:].copy()\n",
        "        force_plot_data.index = X_sample_grouped.columns # Use grouped names for display\n",
        "        shap.force_plot(\n",
        "            explainer_xgb.expected_value,\n",
        "            shap_values_xgb_single,\n",
        "            force_plot_data,\n",
        "            matplotlib=True, # Use matplotlib for static rendering in some environments\n",
        "            show=True # Show the plot\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"SHAP Force Plot failed: {e}\")\n",
        "else:\n",
        "    print(\"\\nCannot generate SHAP Force Plot: X_sample is empty.\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Cosmic Verdict\n",
        "# Print a summary of the model performance comparison.\n",
        "# ---------------------------\n",
        "print(\"\\n--- ⚖️ Cosmic Model Verdict ---\")\n",
        "print(f\"RandomForest Optuna MAE: {mae_rf:.4f}\")\n",
        "print(f\"XGBoost Optuna MAE: {mae_xgb:.4f}\")\n",
        "\n",
        "if mae_rf < mae_xgb:\n",
        "    print(\"✨ RandomForest holds the throne in this dimension (lower MAE).\")\n",
        "elif mae_xgb < mae_rf:\n",
        "    print(\"✨ XGBoost reigns supreme across the melting multiverse (lower MAE).\")\n",
        "else:\n",
        "    print(\"⚖️ Both models are equally balanced in their predictive power.\")"
      ],
      "metadata": {
        "id": "TT86ktmpKbOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna-integration[xgboost]"
      ],
      "metadata": {
        "id": "KoIBwaY9Mblj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PUbaeuHJKbXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbhJMsV5Kba3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qypAfdmXInni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5314bc37"
      },
      "source": [
        "# Task\n",
        "Analyze the provided melting point dataset (\"train.csv\", \"test.csv\", \"sample_submission.csv\"), perform data preprocessing, feature engineering, hyperparameter tuning using Optuna for RandomForest and XGBoost, ensemble modeling, SHAP analysis, add multi-language plot annotations, and generate a submission file in the format of \"sample_submission.csv\". Ensure the notebook is concise, well-labeled, and includes relevant plots and graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3576ada8"
      },
      "source": [
        "## Hyperparameter tuning with optuna\n",
        "\n",
        "### Subtask:\n",
        "Implement more extensive hyperparameter tuning for both RandomForest and XGBoost using Optuna.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "102534e9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code cells have already set up the environment, loaded the data, and performed some initial EDA and baseline modeling. The current subtask is to perform more extensive hyperparameter tuning using Optuna. The existing code cell `n6mjDJxgJuTk` already contains the basic structure for Optuna tuning for both RandomForest and XGBoost. I will modify and extend this cell to perform more trials and ensure the best models are saved. I will also ensure the `y_test` variable is not used in the objective function as it is not available during training/tuning. The objective function should return the cross-validation score on the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "049cdd97"
      },
      "source": [
        "# Extend the Optuna tuning cell (n6mjDJxgJuTk)\n",
        "\n",
        "# --- CV setup ---\n",
        "# Ensure X_train, y_train, X_test, train_df, test_df, groups are available from previous cells.\n",
        "# If 'family' column exists and has multiple unique values in train_df, use GroupKFold.\n",
        "if 'family' in train_df.columns and len(train_df['family'].unique()) > 1:\n",
        "    cv = GroupKFold(n_splits=5)\n",
        "    group_train = train_df['family'].values\n",
        "    print(\"Using GroupKFold for cross-validation.\")\n",
        "else:\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    group_train = None\n",
        "    print(\"Using KFold for cross-validation.\")\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "# ---------------------------\n",
        "# RandomForest Optuna Tuning\n",
        "# ---------------------------\n",
        "def objective_rf(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1500), # Increased range\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 60), # Increased range\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 25), # Increased range\n",
        "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 15), # Increased range\n",
        "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None, 0.5, 0.7, 0.9]), # Added float options\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "    model = RandomForestRegressor(**params)\n",
        "    if group_train is not None:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=mae_scorer, groups=group_train, n_jobs=-1)\n",
        "    else:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=mae_scorer, n_jobs=-1)\n",
        "    return -scores.mean() # Return negative MAE for minimization\n",
        "\n",
        "print(\"\\nStarting Optuna tuning for RandomForest...\")\n",
        "study_rf = optuna.create_study(direction=\"minimize\")\n",
        "study_rf.optimize(objective_rf, n_trials=150, show_progress_bar=True) # Increased trials\n",
        "\n",
        "print(\"\\nBest RF Params:\", study_rf.best_params)\n",
        "print(\"Best RF CV MAE:\", study_rf.best_value)\n",
        "\n",
        "# ---------------------------\n",
        "# XGBoost Optuna Tuning\n",
        "# ---------------------------\n",
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1500), # Increased range\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15), # Increased range\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.2, log=True), # Adjusted range\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0), # Adjusted range\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0), # Adjusted range\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0, 10), # Increased range\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 15), # Increased range\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True), # Added L1 regularization\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True), # Added L2 regularization\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "        \"tree_method\": 'hist' # Use hist for potentially faster training\n",
        "    }\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "    if group_train is not None:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=mae_scorer, groups=group_train, n_jobs=-1)\n",
        "    else:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=mae_scorer, n_jobs=-1)\n",
        "    return -scores.mean() # Return negative MAE for minimization\n",
        "\n",
        "print(\"\\nStarting Optuna tuning for XGBoost...\")\n",
        "study_xgb = optuna.create_study(direction=\"minimize\")\n",
        "study_xgb.optimize(objective_xgb, n_trials=150, show_progress_bar=True) # Increased trials\n",
        "\n",
        "print(\"\\nBest XGB Params:\", study_xgb.best_params)\n",
        "print(\"Best XGB CV MAE:\", study_xgb.best_value)\n",
        "\n",
        "# ---------------------------\n",
        "# Train final models with best params and save\n",
        "# ---------------------------\n",
        "print(\"\\nTraining final models with best parameters...\")\n",
        "best_rf = RandomForestRegressor(**study_rf.best_params, random_state=42, n_jobs=-1)\n",
        "best_rf.fit(X_train, y_train)\n",
        "\n",
        "best_xgb = xgb.XGBRegressor(**study_xgb.best_params, random_state=42, n_jobs=-1, tree_method='hist')\n",
        "best_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Save models\n",
        "os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n",
        "joblib.dump(best_rf, \"/kaggle/working/models/rf_optuna_tuned.joblib\")\n",
        "joblib.dump(best_xgb, \"/kaggle/working/models/xgb_optuna_tuned.joblib\")\n",
        "\n",
        "print(\"\\nSaved best tuned models to /kaggle/working/models/\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}